<!doctype html><html class="theme-next pisces use-motion"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><script></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css"><meta name="keywords" content="Deep Learning,TensorFlow,"><link rel="alternate" href="/atom.xml" title="Yuan's Blog" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1"><meta name="description" content="CS231n 中的概念 公式 Tricks 总结–神经网络（三）神经网络结构  第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。"><meta name="keywords" content="Deep Learning,TensorFlow"><meta property="og:type" content="article"><meta property="og:title" content="CS231n 中的概念 公式 Tricks 总结--神经网络（三）"><meta property="og:url" content="https://www.liuyuan.live/Articles/CS231n3.html"><meta property="og:site_name" content="Yuan&#39;s Blog"><meta property="og:description" content="CS231n 中的概念 公式 Tricks 总结–神经网络（三）神经网络结构  第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909252301479/14909280613993.png"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909287679399.png"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909296119811.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909297033262.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909296924131.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298042017.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298101346.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298674291.png"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909303723235.jpg"><meta property="og:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909304139062.jpg"><meta property="og:updated_time" content="2017-03-31T04:10:03.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS231n 中的概念 公式 Tricks 总结--神经网络（三）"><meta name="twitter:description" content="CS231n 中的概念 公式 Tricks 总结–神经网络（三）神经网络结构  第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。"><meta name="twitter:image" content="http://okn3yh48r.bkt.clouddn.com/media/14909252301479/14909280613993.png"><script type="text/javascript" id="hexo.configuration">var NexT=window.NexT||{},CONFIG={scheme:"Pisces",sidebar:{position:"right",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:void 0,author:"博主"}}</script><link rel="canonical" href="https://www.liuyuan.live/Articles/CS231n3.html"><title> CS231n 中的概念 公式 Tricks 总结--神经网络（三） | Yuan's Blog</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container one-collumn sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader" style="background-image:url(/images/post_bg.jpg)"><div class="header-inner"><a class="site-home" href="/">Yuan's Blog</a><div class="site-meta"><div class="custom-logo-site-title"><div href="/" class="brand"><span class="logo-line-before"><i></i></span> <span class="site-title">Yuan's Blog</span><span class="logo-line-after"><i></i></span></div></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签云</a></li><li class="menu-item menu-item-readinglist"><a href="/readinglist" rel="section"><i class="menu-item-icon fa fa-fw fa-book"></i><br> 书单</a></li><li class="menu-item menu-item-projects"><a href="/projects" rel="section"><i class="menu-item-icon fa fa-fw fa-tasks"></i><br> 项目</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup"> <input type="text" id="local-search-input"><span class="search-icon fa fa-search"></span><div id="local-search-result"></div> <span class="popup-btn-close">close</span></div></div></nav></div><div class="header-post"><div class="post-header"><div class="tags"> <a href="/tags/Deep-Learning/" rel="tag" title="Deep Learning">Deep Learning</a> <a href="/tags/TensorFlow/" rel="tag" title="TensorFlow">TensorFlow</a></div><h1>CS231n 中的概念 公式 Tricks 总结--神经网络（三）</h1><h2 class="subtitle"></h2><div class="post-time"> <span class="post-meta-item-text">发表于</span> <time itemprop="dateCreated" datetime="2017-03-31T11:52:46+08:00" content="2017-03-31" title="2017-03-31 11:52:46">2017-03-31</time></div></div></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><header class="post-header"><h1 class="post-title" itemprop="name headline"> CS231n 中的概念 公式 Tricks 总结--神经网络（三）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time itemprop="dateCreated" datetime="2017-03-31T11:52:46+08:00" content="2017-03-31">2017-03-31</time></span> <span class="post-category">&nbsp; | &nbsp;<span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a href="/categories/Articles/" itemprop="url" rel="index"><span itemprop="name">Articles</span></a></span></span> <span class="post-time">&nbsp; | &nbsp;<span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">字数统计:</span> <span class="post-count">1,384(字)</span></span> <span class="post-time">&nbsp; | &nbsp;<span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">阅读时长:</span> <span class="post-count">5(分)</span></span> <span id="/Articles/CS231n3.html" class="leancloud_visitors" data-flag-title="CS231n 中的概念 公式 Tricks 总结--神经网络（三）">&nbsp; | &nbsp;<span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">热度:</span><span class="leancloud-visitors-count"></span> <span>°C</span></span></div></header><div class="post-body" itemprop="articleBody"><link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="CS231n-中的概念-公式-Tricks-总结–神经网络（三）"><a href="#CS231n-中的概念-公式-Tricks-总结–神经网络（三）" class="headerlink" title="CS231n 中的概念 公式 Tricks 总结–神经网络（三）"></a>CS231n 中的概念 公式 Tricks 总结–神经网络（三）</h1><h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><p><img src="http://okn3yh48r.bkt.clouddn.com/media/14909252301479/14909280613993.png" alt=""></p><ul><li>第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。</li><li>第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。</li></ul><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p><strong>零中心化／均值中心化／均值减法</strong> 在每个维度上都将数据云的中心都迁移到原点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">X -= np.mean(X)</div></pre></td></tr></table></figure><p>？ 这里有个问题 均值中心化能不能提高泛化性能 求大佬解答</p><p><strong>归一化</strong><br>指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。</p><ul><li>第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差</li><li>第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。</li></ul><p><img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909287679399.png" alt=""></p><blockquote><p>一般数据预处理流程：左边：原始的2维输入数据。中间：在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。右边：每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。</p></blockquote><p><strong>PCA和白化（Whitening）</strong><br><strong>PCA</strong>先对数据进行零中心化处理，然后计算协方差矩阵</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># 假设输入数据矩阵X的尺寸为[N x D]</div><div class="line">X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)</div><div class="line">cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵</div></pre></td></tr></table></figure><p><strong>白化</strong>操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。<br>该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。</p><p>wait for update</p><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p><strong>！！！绝对不能全零初始化！！！</strong></p><p><strong>小随机数初始化</strong> 权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来打破对称性。TensorFlow实践的时候，通常使用截断正态分布，然后赋予一个很小的方差。</p><p><strong>偏置（biases）的初始化</strong> 通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。</p><p><strong>批量归一化（Batch Normalization）</strong> 让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。应用这个技巧通常意味着全连接层（或者是卷积层）与激活函数之间添加一个BatchNorm层。</p><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a>正则化Regularization</h3><p><strong>L2正则化</strong><br>对于网络中的每个权重w，向目标函数中增加一个<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909296119811.jpg" alt=""><br>使用L2正则化意味着所有的权重都以w += -lambda * W向着0线性下降。</p><p><strong>L1正则化</strong><br>对于每个w我们都向目标函数增一个<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909297033262.jpg" alt="">。L1和L2正则化也可以进行组合：<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909296924131.jpg" alt=""><br>，这也被称作Elastic net regularizaton。</p><p><strong>最大范式约束（Max norm constraints）</strong><br>要求神经元中的权重向量必须满足<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298042017.jpg" alt="">必须满足<br><img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298101346.jpg" alt=""> ，一般c值为3或者4。</p><p>即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，</p><p><strong>随机失活（Dropout）</strong><br>让神经元以超参数p的概率被激活或者被设置为0。<br><img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909298674291.png" alt=""><br>随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。</p><p>随机失活可以提高泛化能力，避免网络过度学习到数据中的特征。</p><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><h4 id="随机梯度下降以及各种更新方法"><a href="#随机梯度下降以及各种更新方法" class="headerlink" title="随机梯度下降以及各种更新方法"></a>随机梯度下降以及各种更新方法</h4><ul><li>普通更新</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 普通更新</div><div class="line">x += - learning_rate * dx</div></pre></td></tr></table></figure><ul><li>动量更新</li></ul><h4 id="学习率退火"><a href="#学习率退火" class="headerlink" title="学习率退火"></a>学习率退火</h4><p>就是逐渐减小学习率</p><ul><li><p>随步数衰减：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。</p></li><li><p>指数衰减。数学公式是<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909303723235.jpg" alt="">，其中alpha0,k是超参数，t是迭代次数（也可以使用周期作为单位）。</p></li><li>1/t衰减的数学公式是<img src="http://okn3yh48r.bkt.clouddn.com/media/14909281700187/14909304139062.jpg" alt="">，其中alpha0,k是超参数，t是迭代次数。</li></ul><h4 id="逐参数适应学习率方法"><a href="#逐参数适应学习率方法" class="headerlink" title="逐参数适应学习率方法"></a>逐参数适应学习率方法</h4><ul><li>Adagrad</li><li>Adam<br>论文中推荐的参数值eps=1e-8, beta1=0.9, beta2=0.999</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">m = beta1*m + (1-beta1)*dx</div><div class="line">v = beta2*v + (1-beta2)*(dx**2)</div><div class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</div></pre></td></tr></table></figure><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。</p><ul><li>同一个模型，不同的初始化</li><li>在交叉验证中发现最好的模型， 选取其中最好的几个进行集成</li><li>一个模型设置多个记录点</li><li>在训练的时候跑参数的平均值</li></ul> 谢谢观赏~</div><div></div><div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'> <span>赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"> <img id="wechat_qr" src="http://okn3yh48r.bkt.clouddn.com/17-7-16/10132855.jpg" alt="LIU YUAN WeChat Pay"><p>微信打赏</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a><a href="/tags/TensorFlow/" rel="tag"><i class="fa fa-tag"></i> TensorFlow</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/Articles/CS231n2.html" rel="next" title="CS231n 中的概念 公式 Tricks 总结--神经网络（二）"><i class="fa fa-chevron-left"></i> CS231n 中的概念 公式 Tricks 总结--神经网络（二）</a></div><div class="post-nav-prev post-nav-item"> <a href="/C/cpplearning.html" rel="prev" title="C++学习笔记">C++学习笔记<i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="http://okn3yh48r.bkt.clouddn.com/17-7-16/38284782.jpg" alt="LIU YUAN"><p class="site-author-name" itemprop="name">LIU YUAN</p><p class="site-description motion-element" itemprop="description">In solitude, be a multitude to thyself.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives"><span class="site-state-item-count">17</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags"><span class="site-state-item-count">20</span> <span class="site-state-item-name">标签云</span></a></div></nav><div class="music"><iframe frameborder="no" border="0" marginwidth="0" width="298" height="52" src="//music.163.com/outchain/player?type=2&id=16621268&auto=0&height=32"></iframe></div><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a> <a title="收藏到书签，偶尔High一下^_^" rel="alternate" class="mw-harlem_shake_slow wobble shake" href='javascript:(function() { function c() { var e=document.createElement("link"); e.setAttribute("type", "text/css"); e.setAttribute("rel", "stylesheet"); e.setAttribute("href", f); e.setAttribute("class", l); document.body.appendChild(e) } function h() { var e=document.getElementsByClassName(l); for (var t=0; t < e.length; t++) { document.body.removeChild(e[t]) } } function p() { var e=document.createElement("div"); e.setAttribute("class", a); document.body.appendChild(e); setTimeout(function() { document.body.removeChild(e) }, 100) } function d(e) { return { height : e.offsetHeight, width : e.offsetWidth } } function v(i) { var s=d(i); return s.height>e && s.height<n && s.width> t && s.width<r } function m(e) { var t=e; var n=0; while (!!t) { n +=t.offsetTop; t=t.offsetParent } return n } function g() { var e=document.documentElement; if (!!window.innerWidth) { return window.innerHeight } else if (e && !isNaN(e.clientHeight)) { return e.clientHeight } return 0 } function y() { if (window.pageYOffset) { return window.pageYOffset } return Math.max(document.documentElement.scrollTop, document.body.scrollTop) } function E(e) { var t=m(e); return t>= w && t<=b + w } function S() { var e=document.createElement("audio"); e.setAttribute("class", l); e.src=i; e.loop=false; e.addEventListener("canplay", function() { setTimeout(function() { x(k) }, 500); setTimeout(function() { N(); p(); for (var e=0; e < O.length; e++) { T(O[e]) } }, 15500) }, true); e.addEventListener("ended", function() { N(); h() }, true); e.innerHTML=" <p>If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.</p><p>"; document.body.appendChild(e); e.play() } function x(e) { e.className += " " + s + " " + o } function T(e) { e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)] } function N() { var e = document.getElementsByClassName(s); var t = new RegExp("\\b" + s + "\\b"); for (var n = 0; n<e.length; ) { e[n].className=e[n].className.replace(t, "") } } var e=30; var t=30; var n=350; var r=350; var i="//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake.mp3"; var s="mw-harlem_shake_me"; var o="im_first"; var u=["im_drunk", "im_baked", "im_trippin", "im_blown"]; var a="mw-strobe_light"; var f="//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css"; var l="mw_added_css"; var b=g(); var w=y(); var C=document.getElementsByTagName("*"); var k=null; for (var L=0; L < C.length; L++) { var A=C[L]; if (v(A)) { if (E(A)) { k=A; break } } } if (A===null) { console.warn("Could not find a node of the right size. Please try a different page."); return } c(); S(); var O=[]; for (var L=0; L < C.length; L++) { var A=C[L]; if (v(A)) { O.push(A) } } })()'><i class="fa fa-music"></i> High</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/buan1903" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="https://www.linkedin.com/in/buan1903" target="_blank" title="Linkedin"><i class="fa fa-fw fa-linkedin"></i> Linkedin</a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/buan1903" target="_blank" title="知乎"><i class="fa fa-fw fa-globe"></i> 知乎</a></span><span class="links-of-author-item"><a href="https://github.com/buan1903" target="_blank" title="Instagram"><i class="fa fa-fw fa-instagram"></i> Instagram</a></span><span class="links-of-author-item"><a href="mailto:yuanliu699@foxmail.com" target="_blank" title="邮箱"><i class="fa fa-fw fa-envelope-o"></i> 邮箱</a></span><span class="links-of-author-item"><a href="https://github.com/buan1903" target="_blank" title="Facebook"><i class="fa fa-fw fa-facebook"></i> Facebook</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://github.com/hindupuravinash/the-gan-zoo" title="GAN-ZOO" target="_blank">GAN-ZOO</a></li><li class="links-of-blogroll-item"> <a href="https://www.jiqizhixin.com/" title="机器之心" target="_blank">机器之心</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CS231n-中的概念-公式-Tricks-总结–神经网络（三）"><span class="nav-number">1.</span> <span class="nav-text">CS231n 中的概念 公式 Tricks 总结–神经网络（三）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络结构"><span class="nav-number">1.0.1.</span> <span class="nav-text">神经网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据预处理"><span class="nav-number">1.0.2.</span> <span class="nav-text">数据预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#权重初始化"><span class="nav-number">1.0.3.</span> <span class="nav-text">权重初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化Regularization"><span class="nav-number">1.0.4.</span> <span class="nav-text">正则化Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数更新"><span class="nav-number">1.0.5.</span> <span class="nav-text">参数更新</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降以及各种更新方法"><span class="nav-number">1.0.5.1.</span> <span class="nav-text">随机梯度下降以及各种更新方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#学习率退火"><span class="nav-number">1.0.5.2.</span> <span class="nav-text">学习率退火</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逐参数适应学习率方法"><span class="nav-number">1.0.5.3.</span> <span class="nav-text">逐参数适应学习率方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型集成"><span class="nav-number">1.0.6.</span> <span class="nav-text">模型集成</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="copyright"> &copy; 2016 - <span itemprop="copyrightYear">2017</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">LIU YUAN</span></div><div class="powered-by"><i class="fa fa-user-md"></i> <span id="busuanzi_container_site_uv">本站访客数:<span id="busuanzi_value_site_uv"></span></span></div><div class="powered-by"> 由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info"> 主题 - <a class="theme-link" href="https://github.com/BearD01001/hexo-theme-nextd">NexTD</a></div><div class="theme-info"><div class="powered-by"></div> <span class="post-count">博客全站共9.4k字</span></div><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:93326,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:93326,xid:"Articles/CS231n3.html"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/93326/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").fadeToggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!0,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=r.url,i=-1,l=-1,p=-1;if(""!=n&&""!=s&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),i<0&&l<0?c=!1:(l<0&&(l=0),0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+decodeURIComponent(o)+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").fadeOut(function(){$(".popoverlay").remove(),$("body").css("overflow","")})}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script><script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("UBrYnSMKnXcVtXJSVntb2Ler-gzGzoHsz","biKs2aXPr4MT5SzuBMix6Vfr")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){if(0!==e.length){for(c=0;c<e.length;c++){var t=e[c],i=t.get("url"),s=t.get("time"),l=document.getElementById(i);$(l).find(".leancloud-visitors-count").text(s)}for(var c=0;c<n.length;c++){var i=n[c],l=document.getElementById(i),r=$(l).find(".leancloud-visitors-count");""==r.text()&&r.text(0)}}else o.find(".leancloud-visitors-count").text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,l=new AV.ACL;l.setPublicReadAccess(!0),l.setPublicWriteAccess(!0),s.setACL(l),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/javascript" async src="//push.zhanzhang.baidu.com/push.js"></script></body></html>